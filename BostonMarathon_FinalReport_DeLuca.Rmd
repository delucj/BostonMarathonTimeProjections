---
title: "Management of Boston Marathon Finish Line Spectator Density via Machine Learning"
author: "James DeLuca"
date: "11/02/2020"
output: pdf_document
---


```{r Load Packages, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
## By convention, throughout this script I will use single comment symbols "#" to indicated code that has been commented out.
## Where a comment is actually a comment I will preface with double "##" comment symbols
## A docstring is indicated with the "#'" notation
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(lars)) install.packages("lars", repos = "http://cran.us.r-project.org")
if(!require(plot3D)) install.packages("plot3D", repos = "http://cran.us.r-project.org")
if(!require(docstring)) install.packages("docstring", repos = "http://cran.us.r-project.org")
if(!require(svDialogs)) install.packages("svDialogs", repos = "http://cran.us.r-project.org")
if(!require(ggsci)) install.packages("ggsci", repos = "http://cran.us.r-project.org")

library(knitr)
library(dplyr)
library(ggplot2)
library(data.table)
library(tidyverse)
library(caret)
library(ggpubr)
library(stringr)
library(randomForest)
library(lars)
library(plot3D)
library(docstring)
library(svDialogs)
library(ggsci)
```

```{r Define General Functions, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#################################
##
## General Use Functions
## This section contains relatively generalized functions which I may use elsewhere
##
#################################

hms_to_min <- function(race_time){
  #' Reads a timestamp of the form h:mm:ss and returns a floating point number representing the time in minutes
	indicies = str_locate_all(pattern =fixed(':'),race_time)
	hours = as.numeric(substr(race_time,0,indicies[[1]][1]-1))
	minutes = as.numeric(substr(race_time,indicies[[1]][1]+1,indicies[[1]][2]-1))
	seconds = as.numeric(substr(race_time,indicies[[1]][2]+1,indicies[[1]][2]+2))
	time = (60 * hours) + minutes + (seconds / 60)
	return(time)
}

convert_min_to_racetime <- function(time){
  #' This function converts a race time from the units of minutes to a text string in the format h:mm:ss
  
  hours = floor(time / 60)
  minutes = floor((time/60 - floor(time/60)) * 60)
  seconds = round((time - floor(time))*60,0)
  
  # Pad the minutes and seconds with leading zeros so that we don't get results like 3:4:7
  min_disp = str_sub(paste("00",minutes,sep=""),start = -2)
  sec_disp = str_sub(paste("00",seconds,sep=""),start = -2)
  
  return(paste(hours,":",min_disp,":",sec_disp,sep=""))
}

read_csv_from_github <- function(githubURL,file_name) {
  #' Downloads a csv from github and reads the file into a dataframe
	download.file(githubURL,file_name,method = 'curl')
	read.csv(file_name)
}

read_rds_from_github <- function(githubURL,file_name){
  #' Downloads and returns an RDS file
  download.file(githubURL,file_name)
  readRDS(file_name)
}

RMSE <- function(acutal_time, predicted_time){
  #' Takes an set of actual and predicted values and returns the root mean square error, order is not critical
	rmse_dat = data.frame(Actual = acutal_time,Predicted = predicted_time)
	rmse_dat = rmse_dat %>%
		filter(!is.na(Actual)) %>%
		filter(!is.na(Predicted))
	rmse = sqrt(mean((rmse_dat$Actual - rmse_dat$Predicted)^2))
	return(rmse)
}

smart_set_seed <- function(seed_value){
  #' Takes a seed value, checks the version of all and calls set.seed with the appropriate arguments
	if (as.numeric(substr(paste(R.Version()$major,".",R.Version()$minor,sep=""),1,3)) > 3.5) {
		set.seed(seed_value, sample.kind="Rounding") } else {
		set.seed(seed_value)}
}

solve_quadratic <- function(y_data,x_data){
  #' This is a closed form solution to solve for the coefficients of a 2nd order polynomial fit
  #' The input is a pair of vectors, the dependent variable vector is supplied as y-data first
  #' This function will return the x_data value at the minima or maxima of the y_data calculated from the derivative of the polynomial fit
  
   # In the context of this project I'm using this for finding minimum points by passing the smallest value and its neighbors
  # The same function works for local maxima if the maximum is passed with its neighbors
  
  n = nrow(data.frame(y_data))
  xbar = sum(x_data) / n
  ybar = sum(y_data) / n
  x2bar = sum(x_data^2) / n
  
  Sxx = sum((x_data - xbar)^2)
  Sxy = sum((x_data - xbar) * (y_data - ybar))
  Sxx2 = sum((x_data - xbar) * (x_data^2 - x2bar))
  Sx2x2 = sum((x_data^2 - x2bar)^2)
  Sx2y = sum((x_data^2 - x2bar) * (y_data - ybar))
    
  # Coefficients B0, B1, B2 (form y = B0 + B1*x + B2*x^2)
  B2 = (Sx2y*Sxx - Sxy*Sxx2) / ((Sxx*Sx2x2) - (Sxx2**2))
  B1 = (Sxy*Sx2x2 - Sx2y*Sxx2) / ((Sxx*Sx2x2) - (Sxx2**2))
  B0 = ybar - B1*xbar - B2*x2bar
  
  # Derivative: dy/dx = 2*B2*x + B1
  # dy/dx = 0 @ the inflection
  # 0 = 2B2*x + B1 so (-B1 / 2*B2) = x
  x_for_inflection = -B1 / (2 * B2)
  return(x_for_inflection)
}

## This function will make it easy to generate consistent training and testing sets
make_train_test_sets <- function(boston_data,test_size,seed_value){
  #' This purpose of this function is to get training and testing sets from a body of data
  #' The input is a dataframe, the fraction of the data to put into the testing set and a random seed value
  # The function will return a list with the training set at [[1]] and the test set at [[2]]
	
  smart_set_seed(seed_value)
	test_index = createDataPartition(y = boston_data$Final_Time,
		times = 1, p = test_size, list = FALSE)
	boston_train = boston_data[-test_index,]
	boston_test = boston_data[test_index,]
	
	# R does not support multiple return values so I need to put the train and test objects into a list to return
	train_test_sets = list(boston_train,boston_test)
	return(train_test_sets)
}

```

```{r Define Data Cleaning Functions, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Boston Marathon Data Cleaning and Processing Functions
## This section contains the pre-processing functions needed to clean and prepare Boston Marathon results data
##
#########################################################

pre_process_data <- function(boston_data,race_year){
  #' This function is a container for all the Boston Marathon data cleaning functions
  #' The input is a raw Boston Marathon results dataframe which will be returned ready for use with the models in this project

  boston_data = clean_results_data(boston_data)
  boston_data = add_runner_wave(boston_data,race_year)
  boston_data = determine_cutoff_pace(boston_data,race_year)
  boston_data = add_expected_bq_times(boston_data)
  boston_data = add_bq_normalized_times(boston_data)
  boston_data = add_naive_finish_times(boston_data)
  boston_data = add_pace_since_last_split(boston_data)
  boston_data = add_normliazed_paces(boston_data)
  boston_data = filter_bad_columns(boston_data)
  
  return(boston_data)
}

clean_results_data <- function(boston_data){
  #' This function converts the Marathon results from h:m:s format to minutes and filters out corrupted data
  #' Elite (professional) results are removed since they are not representative of the general race population
  #' The only input required is a Boston Marathon results dataframe

  boston_data = boston_data %>%
    mutate(KM05_Time = hms_to_min(X5K),
           KM10_Time = hms_to_min(X10K),
           KM15_Time = hms_to_min(X15K),
           KM20_Time = hms_to_min(X20K),
           Half_Time = hms_to_min(Half),
           KM25_Time = hms_to_min(X25K),
           KM30_Time = hms_to_min(X30K),
           KM35_Time = hms_to_min(X35K),
           KM40_Time = hms_to_min(X40K),
           Final_Time = hms_to_min(Official.Time)) %>%
    filter(!is.na(KM05_Time) &
             !is.na(KM10_Time) &
             !is.na(KM15_Time) &
             !is.na(KM20_Time) &
             !is.na(Half_Time) &
             !is.na(KM25_Time) &
             !is.na(KM30_Time) &
             !is.na(KM35_Time) &
             !is.na(KM40_Time) &
             !is.na(Final_Time)) %>%
    mutate(Bib = as.numeric(Bib)) %>%
    filter(!is.na(Bib) & Bib > 100)
  ## The B.A.A. officially starts qualified (non-professional or sponsored) runners at Bib number 101, some elite bibs are non-numeric
  
  return(boston_data)
}

add_runner_wave <- function(boston_data,race_year){
  #' This function will download the wave-information by year and assign the appropriate wave to each runner
  #' The input required is a Boston Marathon results dataframe and the year of the race
  #' The function will return the dataframe with wave specific information for each runner
  
  ## The wave number and wave-cutoff information is available on the BAA site but I put it into a csv just in case the BAA changes the URL to this information
  wave_info <<- read_csv_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/marathon_wave_information.csv","marathon_wave_information.csv")
  
  ## Since each year the data may be different we'll filter by the race year
  this_year_wave_data = wave_info %>%
    filter(Year == race_year) %>%
    mutate(Wave = as.factor(Wave))
  
  ## From the bib number and the wave data we can assign each runner to one of the 4 waves
  boston_data = boston_data %>%
    filter(Bib > min(this_year_wave_data$Min.Bib.Number)) %>%
    mutate(Wave = as.factor(ifelse(Bib < this_year_wave_data$Min.Bib.Number[2],1,
                                   ifelse(Bib < this_year_wave_data$Min.Bib.Number[3],2,
                                          ifelse(Bib < this_year_wave_data$Min.Bib.Number[4],3,4)))))
  
  ## The wave is an indication of runner qualification time and the fastest and slowest qualification times in each wave.
  this_year_wave_data = this_year_wave_data %>%
    select(Wave,Min.Qual.Time,Max.Qual.Time,Cutoff)
  
  boston_data = left_join(boston_data,this_year_wave_data,by="Wave")
  return(boston_data)
}

determine_cutoff_pace <- function(boston_data,race_year){
  #' This function will download and merge in the qualification standards for the next Boston Marathon
  #' The input is a Boston Marathon results dataframe and the race year
  #' The function will return the dataframe with the times that each runner would need to run to requalify
  #' The qual_info dataframe is assinged with the global variable assignment opperator "<<-" to make it available in other functions without requiring that we return it from this fun
  
  ## The qualification information is available on the BAA site but I put it into a csv just in case the BAA changes the URL to this information
  qual_info <<- read_csv_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/marathon_qualification_standards.csv","marathon_qualification_standards.csv")
  
  ## Since each year the data may be different we'll filter by the race year
  this_year_wave_data = wave_info %>%
    filter(Year == race_year)
  cutoff_modifier = mean(this_year_wave_data$Cutoff)
  
  # Everyone's age is incremented by 1 since the qualification standards are based on what your age will be on the next race year
  next_year_qual_time = ifelse(boston_data$M.F == "M",
                               ifelse(boston_data$Age + 1 < qual_info$Max.Age[1],qual_info$Men[1],
                                      ifelse(boston_data$Age + 1 < qual_info$Max.Age[2],qual_info$Men[2],
                                             ifelse(boston_data$Age + 1 < qual_info$Max.Age[3],qual_info$Men[3],
                                                    ifelse(boston_data$Age + 1 < qual_info$Max.Age[4],qual_info$Men[4],
                                                           ifelse(boston_data$Age + 1 < qual_info$Max.Age[5],qual_info$Men[5],
                                                                  ifelse(boston_data$Age + 1 < qual_info$Max.Age[6],qual_info$Men[6],
                                                                         ifelse(boston_data$Age + 1 < qual_info$Max.Age[7],qual_info$Men[7],
                                                                                ifelse(boston_data$Age + 1 < qual_info$Max.Age[8],qual_info$Men[8],
                                                                                       ifelse(boston_data$Age + 1 < qual_info$Max.Age[9],qual_info$Men[9],
                                                                                              ifelse(boston_data$Age + 1 < qual_info$Max.Age[10],qual_info$Men[10],
                                                                                                     qual_info$Men[11])))))))))),
                               ifelse(boston_data$boston_data$Age + 1 < qual_info$Max.Age[1],qual_info$Women[1],
                                      ifelse(boston_data$Age + 1 < qual_info$Max.Age[2],qual_info$Women[2],
                                             ifelse(boston_data$Age + 1 < qual_info$Max.Age[3],qual_info$Women[3],
                                                    ifelse(boston_data$Age + 1 < qual_info$Max.Age[4],qual_info$Women[4],
                                                           ifelse(boston_data$Age + 1 < qual_info$Max.Age[5],qual_info$Women[5],
                                                                  ifelse(boston_data$Age + 1 < qual_info$Max.Age[6],qual_info$Women[6],
                                                                         ifelse(boston_data$Age + 1 < qual_info$Max.Age[7],qual_info$Women[7],
                                                                                ifelse(boston_data$Age + 1 < qual_info$Max.Age[8],qual_info$Women[8],
                                                                                       ifelse(boston_data$Age + 1 < qual_info$Max.Age[9],qual_info$Women[9],
                                                                                              ifelse(boston_data$Age + 1 < qual_info$Max.Age[10],qual_info$Women[10],
                                                                                                     qual_info$Women[11])))))))))))
  
  ## Each year too many qualifiers try to sign up for the race so runners know that they need to beat their qualification standard by at least the cutoff that they beat to get accepted
  ## I do not know if proximity to a cutoff time will impact runner performance so I'm going to add in this modified feature
  boston_data = boston_data %>% mutate(BQ_Cutoff = (next_year_qual_time - cutoff_modifier))
  
  return(boston_data)
}

add_expected_bq_times <- function(boston_data){
  #' This function adds a check point time that each runner should be at to qualify for the next Boston with even splits
  #' The input is a Boston marathon results data frame which has already been processed with determine_cutoff_pace()
  #' The function returns the mutated dataframe
  
  boston_data = boston_data %>%
    mutate(KM05_BQ_Time = BQ_Cutoff * 3.10686 / 26.2188,
           KM10_BQ_Time = BQ_Cutoff * 6.21371 / 26.2188,
           KM15_BQ_Time = BQ_Cutoff * 9.32057 / 26.2188,
           KM20_BQ_Time = BQ_Cutoff * 12.4274 / 26.2188,
           Half_BQ_Time = BQ_Cutoff * 13.1094 / 26.2188,
           KM25_BQ_Time = BQ_Cutoff * 15.5343 / 26.2188,
           KM30_BQ_Time = BQ_Cutoff * 18.6411 / 26.2188,
           KM35_BQ_Time = BQ_Cutoff * 21.748 / 26.2188,
           KM40_BQ_Time = BQ_Cutoff * 24.8548 / 26.2188)
  
  return(boston_data)
}

add_bq_normalized_times <- function(boston_data){
  #' This function adds features for what % faster or slower each runner is at each split than their qualification time
  #' The input is a boston marathon results data frame which has already been processed with add_expected_bq_times()
  #' The function returns the mutated dataframe
  
  boston_data = boston_data %>%
    mutate(KM05_BQ_Nrm = 100*(KM05_Time - KM05_BQ_Time) / KM05_BQ_Time,
           KM10_BQ_Nrm = 100*(KM10_Time - KM10_BQ_Time) / KM10_BQ_Time,
           KM15_BQ_Nrm = 100*(KM15_Time - KM15_BQ_Time) / KM15_BQ_Time,
           KM20_BQ_Nrm = 100*(KM20_Time - KM20_BQ_Time) / KM20_BQ_Time,
           Half_BQ_Nrm = 100*(Half_Time - Half_BQ_Time) / Half_BQ_Time,
           KM25_BQ_Nrm = 100*(KM25_Time - KM25_BQ_Time) / KM25_BQ_Time,
           KM30_BQ_Nrm = 100*(KM30_Time - KM30_BQ_Time) / KM30_BQ_Time,
           KM35_BQ_Nrm = 100*(KM35_Time - KM35_BQ_Time) / KM35_BQ_Time,
           KM40_BQ_Nrm = 100*(KM40_Time - KM40_BQ_Time) / KM40_BQ_Time)
  
  return(boston_data)
}

add_naive_finish_times <- function(boston_data){
  #' The B.A.A. runner tracking app projects the finish time for each runner at each split to be their average pace to that point extended out over the full course
  #' This function adds the "Naive" prediction at each split. This is the baseline model that we're trying to improve
  #' The function returns the mutated dataframe

    boston_data = boston_data %>%
      mutate(naive_KM05_Finish = KM05_Time * 26.2188 / 3.10686,
             naive_KM10_Finish = KM10_Time * 26.2188 / 6.21371,
             naive_KM15_Finish = KM15_Time * 26.2188 / 9.32057,
             naive_KM20_Finish = KM20_Time * 26.2188 / 12.4274,naive_Half_Finish = Half_Time * 26.2188 / 13.1094,
             naive_KM25_Finish = KM25_Time * 26.2188 / 15.5343,
             naive_KM30_Finish = KM30_Time * 26.2188 / 18.6411,
             naive_KM35_Finish = KM35_Time * 26.2188 / 21.748,
             naive_KM40_Finish = KM40_Time * 26.2188 / 24.8548)

    return(boston_data)
}

add_pace_since_last_split <- function(boston_data){
  #' This function adds the pace that each runner ran from the last check-point to the current check-point
  #' These features are to let the model learn what trends in a runner's pace mean towards their finish time
  #' The input is a Boston Marathon results dataframe
  #' The function returns the mutated dataframe
  
  boston_data = boston_data %>%
    mutate(PaceAt5K = KM05_Time / 3.10686,
           PaceAt10K = (KM10_Time - KM05_Time) / (6.21371 - 3.10686),
           PaceAt15K = (KM15_Time - KM10_Time) / (9.32057 - 6.21371),
           PaceAt20K = (KM20_Time - KM15_Time) / (12.4274 - 9.32057),
           PaceAtHalf = (Half_Time - KM20_Time) / (13.1094 - 12.4274),
           PaceAt25K = (KM25_Time - Half_Time) / (15.5343 - 13.1094),
           PaceAt30K = (KM30_Time - KM25_Time) / (18.6411 - 15.5343),
           PaceAt35K = (KM35_Time - KM30_Time) / (21.748 - 18.6411),
           PaceAt40K = (KM40_Time - KM35_Time) / (24.8548 - 21.748))
  
  return(boston_data)
}

add_normliazed_paces <- function(boston_data){
  #' For each split after 10km this function will add the ratio of the runners most recent pace to their average pace over the first 10km
  #' This function is intended to try to simplify the random forest regression trees that look for slowdown effects
  #' The input is a Boston Marathon results dataframe, the return is the mutated dataframe

    boston_data = boston_data %>%
      mutate(NrmPace_15K = 6.21371 * PaceAt15K / KM10_Time,
             NrmPace_20K = 6.21371 * PaceAt20K / KM10_Time,
             NrmPace_Half = 6.21371 * PaceAtHalf / KM10_Time,
             NrmPace_25K = 6.21371 * PaceAt25K / KM10_Time,
             NrmPace_30K = 6.21371 * PaceAt30K / KM10_Time,
             NrmPace_35K = 6.21371 * PaceAt35K / KM10_Time,
             NrmPace_40K = 6.21371 * PaceAt40K / KM10_Time,)
    
    return(boston_data)
}

add_mid_race_miss <- function(boston_data){
  #' We build a series of linear regression models to predict when each runner will get to the next check-point based on the last check-point
  #' The runner's residual to that prediction is important in predicting the final time
  #' This function takes a dataframe and gets the runner's residuals and ratios at each check-point for use in the random forest models
  #' The function returns the mutated dataframe
  
  ## The additive random forest models look at the residuals (actual - linear prediction) from check-point to check-point
  boston_data$MissAt10k_add = boston_data$KM10_Time - predict(pred10k_lm,newdata = boston_data)
  boston_data$MissAt15k_add = boston_data$KM15_Time - predict(pred15k_lm,newdata = boston_data)
  boston_data$MissAt20k_add = boston_data$KM20_Time - predict(pred20k_lm,newdata = boston_data)
  boston_data$MissAt_Half_add = boston_data$Half_Time - predict(pred_half_lm,newdata = boston_data)
  boston_data$MissAt25k_add = boston_data$KM25_Time - predict(pred25k_lm,newdata = boston_data)
  boston_data$MissAt30k_add = boston_data$KM30_Time - predict(pred30k_lm,newdata = boston_data)
  boston_data$MissAt35k_add = boston_data$KM35_Time - predict(pred35k_lm,newdata = boston_data)
  boston_data$MissAt40k_add = boston_data$KM40_Time - predict(pred40k_lm,newdata = boston_data)
  
  ## The multiplicative random forest models look at the ratios (actual / linear prediction) from check-point to check-point
  boston_data$MissAt10k_mult = boston_data$KM10_Time / predict(pred10k_lm,newdata = boston_data) 
  boston_data$MissAt15k_mult = boston_data$KM15_Time / predict(pred15k_lm,newdata = boston_data)
  boston_data$MissAt20k_mult = boston_data$KM20_Time / predict(pred20k_lm,newdata = boston_data)
  boston_data$MissAt_Half_mult = boston_data$Half_Time / predict(pred_half_lm,newdata = boston_data)
  boston_data$MissAt25k_mult = boston_data$KM25_Time / predict(pred25k_lm,newdata = boston_data)
  boston_data$MissAt30k_mult = boston_data$KM30_Time / predict(pred30k_lm,newdata = boston_data)
  boston_data$MissAt35k_mult = boston_data$KM35_Time / predict(pred35k_lm,newdata = boston_data)
  boston_data$MissAt40k_mult = boston_data$KM40_Time / predict(pred40k_lm,newdata = boston_data)
  
  return(boston_data)
}

filter_bad_columns <- function(boston_data){
  #' The web-scrape sometimes returns some un-identified columns
  #' This function strips them out so that we can do an effective rbind() when creating combined training/testing sets
  #' The function takes in a dataframe of Boston Marathon results and returns the data-frame without the bad columns

  ## It only really matters that we do this for the 2015 and 2016 results so that we can bind the dataframes to make composite training and testing sets
  ## "X" is present in both 2015 but only 2016 has "X.1". These features do not appear to hold any useful information but can get in the way of the bind so I'm throwing them away
  ## I didn't see an "X.2" feature but the code is trivial and the time impact is near zero to have the check and removal so I added the check as buffer
  
  if("X" %in% colnames(boston_data)) {
    boston_data = boston_data %>%
      select(!X)
  }
  if("X.1" %in% colnames(boston_data)) {
    boston_data = boston_data %>%
      select(!X.1)
  }
  if("X.2" %in% colnames(boston_data)) {
    boston_data = boston_data %>%
      select(!X.2)
  }
  return(boston_data)
}

get_naive_performance <- function(boston_data){
  #' This function takes a dataframe of pre-processed Boston Marathon results
  #' This function will return a dataframe with the RMS at each split for the BAA prediction
  #' The predictions all just assume constant pace over the entire race equal to the pace at the check-point
  
  naive_rmse_05km = boston_data %>% filter(!is.na(naive_KM05_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM05_Finish))
  naive_rmse_10km = boston_data %>% filter(!is.na(naive_KM10_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM10_Finish))
  naive_rmse_15km = boston_data %>% filter(!is.na(naive_KM15_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM15_Finish))
  naive_rmse_20km = boston_data %>% filter(!is.na(naive_KM20_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM20_Finish))
  naive_rmse_half = boston_data %>% filter(!is.na(naive_Half_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_Half_Finish))
  naive_rmse_25km = boston_data %>% filter(!is.na(naive_KM25_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM25_Finish))
  naive_rmse_30km = boston_data %>% filter(!is.na(naive_KM30_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM30_Finish))
  naive_rmse_35km = boston_data %>% filter(!is.na(naive_KM35_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM35_Finish))
  naive_rmse_40km = boston_data %>% filter(!is.na(naive_KM40_Finish)) %>%
    summarize(ModRMSE = RMSE(Final_Time,naive_KM40_Finish))
  
  # Bind the results to return as a dataframe
  # The distance vector is the distance in KM of each split
  naive_performance = data.frame(
    cbind(Distance = as.numeric(c(5,10,15,20,21.0975,25,30,35,40)),
          Naive.RMSE = as.numeric(c(naive_rmse_05km,naive_rmse_10km,naive_rmse_15km,
                                    naive_rmse_20km,naive_rmse_half,naive_rmse_25km,
                                    naive_rmse_30km,naive_rmse_35km,naive_rmse_40km))))
  
  return(naive_performance)
}

get_linear_performance <- function(boston_data){
  #' This function takes a dataframe of pre-processed Boston Marathon results
  #' This function will return a dataframe with the RMSE at each split for the linear model predictions
  #' All predictions are multi-variable linear regression model predictions trained on 2015 and 2016 data
  
  lm_rmse_05km = RMSE(boston_data$Final_Time,predict(mod_lm_05,boston_data))
  lm_rmse_10km = RMSE(boston_data$Final_Time,predict(mod_lm_10,boston_data))
  lm_rmse_15km = RMSE(boston_data$Final_Time,predict(mod_lm_15,boston_data))
  lm_rmse_20km = RMSE(boston_data$Final_Time,predict(mod_lm_20,boston_data))
  lm_rmse_half = RMSE(boston_data$Final_Time,predict(mod_lm_Half,boston_data))
  lm_rmse_25km = RMSE(boston_data$Final_Time,predict(mod_lm_25,boston_data))
  lm_rmse_30km = RMSE(boston_data$Final_Time,predict(mod_lm_30,boston_data))
  lm_rmse_35km = RMSE(boston_data$Final_Time,predict(mod_lm_35,boston_data))
  lm_rmse_40km = RMSE(boston_data$Final_Time,predict(mod_lm_40,boston_data))
  
  # Bind the results to return as a dataframe
  # The distance vector is the distance in KM of each split
  linear_performance = data.frame(
    cbind(Distance = as.numeric(c(5,10,15,20,21.0975,25,30,35,40)),
          Linear.RMSE = as.numeric(c(lm_rmse_05km,lm_rmse_10km,lm_rmse_15km,
                                     lm_rmse_20km,lm_rmse_half,lm_rmse_25km,
                                     lm_rmse_30km,lm_rmse_35km,lm_rmse_40km))))
  
  return(linear_performance)
}

get_rf_performance <- function(boston_data){
  #' This function takes a dataframe of pre-processed Boston Marathon results
  #' This function will return a dataframe with the RMSE at each split for the Random Forest model predictions
  #' The predict_rf* functions take care of mixing the two Random Forest models with the linear regression model
  
  rf_rmse_05km = RMSE(boston_data$Final_Time,predict_rf_05km(boston_data))
  rf_rmse_10km = RMSE(boston_data$Final_Time,predict_rf_10km(boston_data))
  rf_rmse_15km = RMSE(boston_data$Final_Time,predict_rf_15km(boston_data))
  rf_rmse_20km = RMSE(boston_data$Final_Time,predict_rf_20km(boston_data))
  rf_rmse_half = RMSE(boston_data$Final_Time,predict_rf_half(boston_data))
  rf_rmse_25km = RMSE(boston_data$Final_Time,predict_rf_25km(boston_data))
  rf_rmse_30km = RMSE(boston_data$Final_Time,predict_rf_30km(boston_data))
  rf_rmse_35km = RMSE(boston_data$Final_Time,predict_rf_35km(boston_data))
  rf_rmse_40km = RMSE(boston_data$Final_Time,predict_rf_40km(boston_data))
  
  # Bind the results to return as a dataframe
  # The distance vector is the distance in KM of each split
  rf_performance = data.frame(
    cbind(Distance = as.numeric(c(5,10,15,20,21.0975,25,30,35,40)),
          RF.RMSE = as.numeric(c(rf_rmse_05km,rf_rmse_10km,rf_rmse_15km,
                                 rf_rmse_20km,rf_rmse_half,rf_rmse_25km,
                                 rf_rmse_30km,rf_rmse_35km,rf_rmse_40km))))
  
  return(rf_performance)
}

setup_for_rf_tune_for <- function(){
  #' This function will setup the nt (number of trees) and reset the rmses vector to an empty vector
  #' This function is used to prepare for the nodesize tuning loop for the Random Forest models used in this project
  #' This function edits global variables so no return is necessary
  
  nt <<- 75
  rmses <<- vector(mode="numeric",length=0)
}

setup_for_rf_tune_while <- function(nodes,rmses){
  #' This function sets old_rmse, old_nt and new_rmse values based on the output of the nodesize tuning for loops
  #' This function is used to initialize these values for use in the while loop that grows the random forest to an optimized size
  #' The input is the nodesize vector and the vector of RMSEs that correspond to those node sizes
  #' This function edits global variables so no return is necessary
  
  ## Node size is optimized from the for loop. I'm using the solve_quadratic function to find the best nodesize
  ## If the best position is at a boundary I just use that boundary
  if(nodes[which.min(rmses)] == max(nodes)) {
    ns = max(nodes)
  } else if (nodes[which.min(rmses)] == min(nodes)) {
    ns = min(nodes)
  } else {
    ns <<- round(solve_quadratic(data.frame(c(rmses[which.min(rmses)-1],rmses[which.min(rmses)],rmses[which.min(rmses)+1])),
                                 data.frame(c(nodes[which.min(rmses)-1],nodes[which.min(rmses)],nodes[which.min(rmses)+1]))),0)
  }
  
  old_rmse <<- min(rmses)	# We will start with trying to optimize the model coming out of node-size tuning
  old_nt <<- 75			# We'll always start at 75 trees
  new_rmse <<- 0			# This ensures we will always try at least one iteration of the while loop
}

```

```{r Training Data Download, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Training Data Download
## I have placed copies of the data collected from Kaggle in a GitHub repository
## This is to ensure that the data being pulled for this analysis does not change over time
##
#########################################################

bm_2015_results <- read_csv_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/marathon_results_2015.csv","marathon_results_2015.csv")
bm_2016_results <- read_csv_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/marathon_results_2016.csv","marathon_results_2016.csv")

## Set aside an unprocessed head from one of the results files for the report
bm_2015_knit <- head(bm_2015_results)

```

```{r Training Data Pre-Process, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Pre-Process the Training Data
##
#########################################################

## This just calls the function which will call all the necessary cleaning and processing functions
bm_2015_results <- pre_process_data(bm_2015_results,2015)
bm_2016_results <- pre_process_data(bm_2016_results,2016)
```



```{r Model Prediction Functions, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Model Prediction Functions
## This block will define functions which make final time predictions at each split
##
#########################################################

predict_rf_05km <- function(boston_data){
  #' This function predicts the final time at 5km
  #' The input is a pre-processed Boston Marathon Results data frame
  #' The prediction is the average of the linear model + a random forest residual and the linear model times a random forest ratio
  #' The function will return a vector of predicted values
   
  ## Bias is reduced with a linear model
  lin_preds = predict(mod_lm_05,newdata = boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_05k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_05k_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_10km <- function(boston_data){
  #' This function predicts the final time at 10km
  #' The input is a pre-processed Boston Marathon Results data frame
  #' The prediction is the average of the linear model + a random forest residual and the linear model times a random forest ratio
  #' The function will return a vector of predicted values
  
  ## Bias is reduced with a linear model
  lin_preds = predict(mod_lm_10,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_10k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_10k_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_15km <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at 15km

  ## Bias is reduced with a linear model
  lin_preds = predict(mod_lm_15,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_15k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_15k_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_20km <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at 20km
  
  ## Bias is reduced with a linear model  
  lin_preds = predict(mod_lm_20,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_20k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_20k_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_half <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at the half marathon

  ## Bias is reduced with a linear model 
  lin_preds = predict(mod_lm_Half,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_half_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_half_mult,newdata = boston_data)
  
  # Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_25km <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at 25km
  
  ## Bias is reduced with a linear model 
  lin_preds = predict(mod_lm_25,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_25k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_25_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_30km <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at 30km
  
  ## Bias is reduced with a linear model 
  lin_preds = predict(mod_lm_30,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_30k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_30_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_35km <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at 35km
  
  ## Bias is reduced with a linear model
  lin_preds = predict(mod_lm_35,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_35k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_35_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

predict_rf_40km <- function(boston_data){
  #' See the docstring from predict_rf10km() the only difference is that this predicts at 40km
  
  ## Bias is reduced with a linear model
  lin_preds = predict(mod_lm_40,boston_data)
  
  ## Variance is modeled with two different random forest models
  add_resids = predict(mod_rf_40k_add,newdata = boston_data)
  mult_ratio = predict(mod_rf_40_mult,newdata = boston_data)
  
  ## Each random forest model adjusts the linear model prediction and the average of the two models is returned
  preds = ((lin_preds + add_resids) + (lin_preds * mult_ratio)) / 2
  return(preds)
}

```


```{r Plot Functions, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Data Visualization Functions
## I collected most of the data visualizations into this block
## The purpose of making these functions is so that I can easily pass in new data without re-coding the formatting
## These functions have some modified aesthetics from the R-Script for better display in the PDF format
##
#########################################################

plot_finish_gender_distribution <- function(boston_data,race_year){
  #' This function will return a plot object that will visualize the distribution of race finish times by sex
  #' The input is a Boston Marathon data frame, it is not necessary that it be pre-processed

  ## Calculate the mean and standard deviation for the plot title
  mean_finish <- round(mean(boston_data$Final_Time),2)
	finish_sigma <- round(sd(boston_data$Final_Time),2)
	
	boston_data %>%
	  mutate(M.F = ifelse(M.F == "F","Female","Male")) %>%
	  ggplot() +
	  geom_histogram(aes(x=Final_Time,fill=M.F),color="black",binwidth=5) +
	  xlab("Actual Finish Time (minutes)") +
	  ggtitle(paste("Distribution of Finish Times by Sex |",race_year,
	                "Mean =",mean_finish,"Sigma =",finish_sigma)) +
	  guides(fill=guide_legend(title="Male/Female")) +
	  xlim(c(150,400)) + scale_fill_igv()
}

plot_finish_by_bib <- function(boston_data,race_year){
  #' This function will return a plot object that will visualize the relationship between Bib number and finish time
  #' The input is a Boston Marathon data frame, the data must be pre-processed to get wave and redact missing values
  
	boston_data %>%
    mutate(M.F = ifelse(M.F == "F","Female","Male")) %>%
    group_by(M.F) %>%
    ggplot() + geom_point(aes(x=Bib,y=Final_Time,color=as.factor(Wave))) +
    guides(color=guide_legend(title="Wave")) +
    facet_grid(cols = vars(M.F)) +
    xlab("Bib Number") + ylab("Final Time (minutes)") +
    ggtitle(paste("Boston Non-Elite Bib Race Performance",race_year)) +
    ylim(c(0,500)) + scale_color_igv()
}

plot_gender_split_by_bib <- function(boston_data,race_year){
  #' This function will return a plot object that will visualize how the gender split changes through the bib-numbers
  #' The input is a Boston Marathon data frame, the data must be pre-processed to get wave and redact missing values
  
	boston_data %>%
    mutate(IsFemale = ifelse(M.F == "F",1,0)) %>%
    ggplot(aes(x=Bib,y=IsFemale,color=as.factor(Wave))) + geom_smooth() +
    guides(color = guide_legend(title="Wave")) +
    xlab("Bib Number") + ylab("Proportion Female") +
    ggtitle(paste("Gender Split by Bib Number and Wave |",race_year)) +
    ylim(c(0,1)) + scale_color_igv()
}

plot_finish_by_Half_pace <- function(boston_data,race_year){
  #' This function will return a plot object that will visualize the linear relationship between race time at the half and finish time
  #' The input is a Boston Marathon data frame, the data must be pre-processed to get wave and redact missing values
  
	boston_data %>%
    mutate(M.F = ifelse(M.F == "F","Female","Male")) %>%
    group_by(M.F) %>%
    ggplot() + geom_point(aes(x=PaceAtHalf,y=Final_Time,color=as.factor(Wave))) +
    guides(color=guide_legend(title="Wave")) +
    facet_grid(cols = vars(M.F)) +
    xlab("Pace at Half (minutes/mile)") + ylab("Final Time (minutes)") +
    ggtitle(paste("Boston Non-Elite Bib Race Performance - Finish vs. Start",race_year)) +
    ylim(c(100,500)) + xlim(c(5,20)) + scale_color_igv()
}

plot_neg_pos_split <- function(boston_data,race_year){
  #' This function will create a plot object will visualize the distribution of how much runners slowed down in the second half of the race
  #' The input is a Boston Marathon data frame, the data must be pre-processed to get wave and redact missing values
  
  boston_data %>%
    mutate(M.F = ifelse(M.F == "F","Female","Male")) %>%
    mutate(SecondHalfSplit = (Final_Time - Half_Time) / Half_Time) %>%
    group_by(M.F) %>%
    ggplot() + geom_histogram(aes(x=SecondHalfSplit,fill=as.factor(Wave)),color="black",binwidth = 0.01) +
    guides(fill=guide_legend(title="Wave")) +
    facet_grid(cols = vars(M.F)) +
    xlab("Ratio of Second Half Time to First Half Time") +
    ggtitle(paste("Boston Non-Elite Bib Race Second Half Split",race_year)) +
    xlim(c(0.9,1.5)) + scale_fill_igv()
}

bib_age_surface <- function(boston_data,race_year,include_wave4,p){
  #' This function will generate a surface plot of Finish Time vs. Age and Bib number
  #' The input is a pre-processed Boston Marathon dataframe, the race year for the title and a TRUE/FALSE flag
  #' If the TRUE/FALSE flag is flase the wave 4 data will not be flagged
  
  y = boston_data$Age
  x = boston_data$Bib / 1000
  z = boston_data$Final_Time / 60
  wave = as.numeric(boston_data$Wave)
  df = data.frame(cbind(x,y,z,wave))
  if (include_wave4){
    x = df$x
    y = df$y
    z = df$z
  } else {
    df = subset(df,df$wave < 4)
    x = df$x
    y = df$y
    z = df$z
  }

## Compute the linear regression (z = ax + by + d)
grid.lines = 25
fit <- lm(z ~ poly(x,1) + poly(y,1))
## predict values on regular xy grid
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
fitpoints <- predict(fit)

scatter3D(x, y, z,
	bty="g", alpha = 0.2, phi = p,
	clab = c("Finish Time","(min)"),
	ticktype = "detailed",
	ylab = "Age (Years)",
	xlab = "Bib Number / 1000", zlab = "Finish Time (hours)",
	surf = list(x = x.pred, y = y.pred, z = z.pred,
		facets = NA, fit = fitpoints),
	main = paste(c("Finish Time by Bib and Age, Boston ",race_year)))
}


bib_pace_surface <- function(boston_data,race_year){
  #' This function will generate a surface plot of Finish/Half Time vs. Bib and Half/10k Pace number
  #' The input is a pre-processed Boston Marathon dataframe, the race year for the title and a TRUE/FALSE flag
  #' If the TRUE/FALSE flag is flase the wave 4 data will not be flagged
  
  y = boston_data$Age
  x = boston_data$NrmPace_Half
  z = boston_data$Final_Time / boston_data$Half_Time
  df = data.frame(cbind(x,y,z))
  df = subset(df,df$x < 1.2)
  df = subset(df,df$z < 3)
  x = df$x
  y = df$y
  z = df$z

## Compute the linear regression (z = ax + by + d)
grid.lines = 25
fit <- lm(z ~ poly(x,2) + poly(y,2))
## predict values on regular xy grid
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
fitpoints <- predict(fit)

scatter3D(x, y, z,
	bty="g", phi = 10, alpha = 0.2,
	clab = "Finish Ratio",
	ticktype = "detailed",
	ylab = "Age (years)",
	xlab = "Half/10k Pace", zlab = "Finish/Half Time",
	cex = 0.75,cey = 0.75,cez = 0.75,
	surf = list(x = x.pred, y = y.pred, z = z.pred,
		facets = NA, fit = fitpoints),
	main = paste("Slowdown vs Age, Boston ",race_year))
}

bib_time_surface <- function(boston_data,race_year,include_wave4){
  #' This function will generate a surface plot of Finish/Half Time vs. Bib and Half/10k Pace number
  y = boston_data$Bib
  x = boston_data$Half_Time
  z = boston_data$Final_Time
  wave = as.numeric(boston_data$Wave)
  df = data.frame(cbind(x,y,z,wave))
  if (include_wave4){
    x = df$x
    y = df$y
    z = df$z
  } else {
    df = subset(df,df$wave < 4)
    x = df$x
    y = df$y
    z = df$z
  }

## Compute the linear regression (z = ax + by + d)
grid.lines = 25
fit <- lm(z ~ poly(x,2) + poly(y,2))
## predict values on regular xy grid
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
fitpoints <- predict(fit)

scatter3D(x, y, z,
	bty="g", phi = 10, alpha = 0.2,
	clab = c("Finish Time","(min)"),
	ticktype = "detailed",
	ylab = "Bib",
	xlab = "Half Marathon Time (min)", zlab = "Finish Time (min)",
	surf = list(x = x.pred, y = y.pred, z = z.pred,
		facets = NA, fit = fitpoints),
	main = paste("Finish vs. Bib and Half Time, Boston ",race_year))
}

bib_5ktime_surface <- function(boston_data,race_year,include_wave4){
  #' This function will generate a surface plot of Finish/Half Time vs. Bib and Half/10k Pace number
  y = boston_data$Bib
  x = boston_data$KM05_Time
  z = boston_data$Final_Time
  wave = as.numeric(boston_data$Wave)
  df = data.frame(cbind(x,y,z,wave))
  if (include_wave4){
    x = df$x
    y = df$y
    z = df$z
  } else {
    df = subset(df,df$wave < 4)
    x = df$x
    y = df$y
    z = df$z
  }

## Compute the linear regression (z = ax + by + d)
grid.lines = 25
fit <- lm(z ~ poly(x,2) + poly(y,2))
## predict values on regular xy grid
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
fitpoints <- predict(fit)

scatter3D(x, y, z,
	bty="g", phi = 10, alpha = 0.2,
	clab = c("Finish Time","(min)"),
	ticktype = "detailed",
	ylab = "Bib",
	xlab = "5km Time (min)", zlab = "Finish Time (min)",
	surf = list(x = x.pred, y = y.pred, z = z.pred,
		facets = NA, fit = fitpoints),
	main = paste("Finish vs. Bib and 5km Time, Boston ",race_year))
}

plot_splis_dist_5k_addative <- function(boston_data,race_year){
  #' This function will return a plot of the distribution of runner slow-downs from the 5km point
  #' The slow down is in the units of minutes for this plot
  boston_data %>% mutate(naive_residuals_5k = Final_Time - naive_KM05_Finish,
                         M.F = ifelse(M.F == "F","Female","Male")) %>%
    ggplot() + geom_histogram(aes(x=naive_residuals_5k,fill=M.F),binwidth=2.5) +
    guides(fill=guide_legend(title="Gender")) + xlab("Final Time - est Final Time from 5km") +
    ylab("Count") + ggtitle(paste("Distribution of Slow Down from 5KM",race_year,"Boston Marathon")) +
    xlim(c(-50,150)) + scale_fill_igv()
}

plot_splis_dist_5k_multiplicative <- function(boston_data,race_year,bw){
  #' This function will return a plot of the distribution of runner slow-downs from the 5km point
  #' The slow down is unitless, it is the ratio of final pace to 5km pace
  boston_data %>% mutate(naive_multiplier_5k = (Final_Time / 26.2188) / (KM05_Time / 3.10686),
                         M.F = ifelse(M.F == "F","Female","Male")) %>%
    ggplot() + geom_histogram(aes(x=naive_multiplier_5k,fill=M.F),binwidth=bw) +
    guides(fill=guide_legend(title="Gender")) + xlab("Ratio: Final Pace / Pace at 5KM") +
    ylab("Count") + ggtitle(paste("Distribution of Slow Down from 5KM",race_year,"Boston Marathon")) +
    xlim(c(0.75,1.75)) + scale_fill_igv()
}

plot_resids_by_age_additive <- function(dist,boston_data,race_year,naive){
  #' This function visualizes the additive residuals to the naive 15km prediction by age, gender and wave
  #' Function input is the distance of the split, the data-set, the race year and the naive predictions
  #' Output is a plot of the residuals to the naive (B.A.A.) prediction by age, split by sex
  boston_data %>%
    mutate(naive_residuals = Final_Time - naive, M.F = ifelse(M.F == "F","Female","Male")) %>%
    group_by(M.F) %>%
    ggplot() + geom_point(alpha = 0.05, aes(x=as.numeric(Age),y=naive_residuals,color=as.factor(Wave))) +
    geom_smooth(se = F,span = 0.2,aes(x=as.numeric(Age),y=naive_residuals,color=as.factor(Wave))) +
    xlab("Runner Age (years)") + ylab(paste("Residual at",dist, "(min)")) +
    ggtitle(paste("Residuals to Naieve Prediction at", dist,"by Age and Gender")) + 
    guides(color=guide_legend(title="Wave")) +
    facet_grid(cols = vars(M.F)) +
    ylim(c(-100,100)) + scale_color_igv()
}

plot_resids_by_age_multiplicative <- function(dist,boston_data,race_year,pace){
  #' This function visualizes the additive residuals to the naive 15km prediction by age, gender and wave
  #' Function input is the distance of the split, the data-set, the race year and the pace up to the split
  #' Output is a plot of the residuals to the naive (B.A.A.) prediction by age, split by sex
  boston_data %>%
    mutate(naive_multiplier = (Final_Time / (26.2188 * pace)), M.F = ifelse(M.F == "F","Female","Male")) %>%
    group_by(M.F) %>%
    ggplot() + geom_point(alpha = 0.05, aes(x=as.numeric(Age),y=naive_multiplier,color=as.factor(Wave))) +
    geom_smooth(se = F,span = 0.2,aes(x=as.numeric(Age),y=naive_multiplier,color=as.factor(Wave))) +
    xlab("Runner Age (years)") + ylab(paste("Finish Pace Ratio at",dist)) +
    ggtitle(paste("Final Pace Ratio:", dist,",",race_year)) + 
    guides(color=guide_legend(title="Wave")) +
    facet_grid(cols = vars(M.F)) +
    ylim(c(0.9,1.25)) + scale_color_igv()
}

plot_rf_imp <- function(model,title){
  rf_mod_imp = varImp(model)
  plot(rf_mod_imp, main = title,
       xlab="Feature Importance")
}

```


# Executive Summary, Introduction and Data Overview

## Executive Summary

Each year the Boston Marathon draws some 30,000 runners and an estimated 500,000 spectators, injecting an estimated [$200 million into the Boston economy](https://www.baa.org/2019-boston-marathon-injects-more-200-million-greater-boston-economy) each year. In 2013, the spectator density at the finish line made the race a target for a heinous terrorist attack. In 2020 the marathon had to be entirely canceled (for the first time since its 1897 inception) due to the disease transmission risks associated with high population densities and the COVID-19 epidemic. The 2021 Boston Marathon has already been postponed for similar COVID transmission fears. For the Boston Marathon and other [Marathon Majors](https://www.worldmarathonmajors.com/) to survive and chart a course forward it will become necessary to simultaneously manage spectator density at key viewing locations while still providing opportunities for family and friends to support the runners on the course, especially at the finish line.

For a race organizer to restrict access to an area of the race course, allowing in only family and friends of runners who are expected to finish within a certain window, the organizers need a good estimate of each runner's finish time and a relatively narrow confidence interval within which they are expected to finish. That confidence interval is derived from the RMSE of the model being used to predict the runner's finish time.

This paper describes the development of a machine learning algorithm which improves upon the existing method used by the Boston Athletic Association (B.A.A.) to project runner finish times. The existing method is to place check-point mats at regular intervals throughout the 42.2km race course. As each runner passes a check-point the runner's average pace from the start to that check-point is calculated and the Final Time is projected by presumption that the runner will complete the entire race at that pace.

The output of this work provides improved finish time predictions that race organizes can use to gate spectator access to restricted areas such as the finish line. The performance of the model is judged by the percent reduction in the RMSE (Root Mean Square Error) calculated at each of the race check-points for the optimized model compared to the B.A.A. baseline projections.

The model described in this paper is trained and tested using data from the 2015 and 2016 Boston Marathons. The performance of this model is validated on the entire list of 2017 Boston Marathon results. In validation this model provides an average improvement in RMSE of **40%** with a peak improvement of **~47.5%** near the Half Marathon check-point.

\newpage
## Introduction to the Dataset

Each year the B.A.A. publishes the results of the Boston Marathon to their website. The results from 2015, 2016 and 2017 were scraped from the official website by rojour at [Kaggle](https://www.kaggle.com/rojour/boston-results). Many thanks to rojour for his work preparing this data set and providing the Python notebooks to replicate this scraping for other years. The web-scraping notebook prepares csv format file which are used in this project. The features available in this data set are:

+ **Bib:** The bib is the number worn by the runner. For most runners, this gives an indication of the runner's qualification time. Some runners enter the marathon via charity or club entry so their bib numbers do not hold any    information about the runner's typical speed. 
+ **Name:** This is the name of the runner; while useful for runners and their friends but is not of interest for this project.
+ **Age:** This is the age of the runner on the day of the race in years.
+ **M/F:** The B.A.A. classified every runner as either male "M" or female "F". This selection matches the gender selected by the runner when submitting a qualifying race performance for entry.
+ **City:** This is the city of the runner's mailing address.
+ **State:** For runners from the United States, this feature is the State from the runner's mailing address
+ **Country:** This is the Country from the runner's mailing address.
+ **Citizen:** If a runner is a citizen of different country than the mailing address then this feature lists the runner's country of citizenship.
+ **5K, 10K, 15K, 20K, 25K, 30K, 35K, 40K:** These are the elapsed times for each runner from the start to each of the timing mats every 5km through the course
+ **Half, Official Time:** These are the elapsed times when each runner passes the half marathon line and the finish line. The **Official Time** is the dependent variable which we will predict at each Check-Point.
+ **Pace:** The pace is average pace in time per mile of each runner up to the latest timing mat which has been crossed. In the scraped data this is always the average pace for the entire race but if the data are sampled through the B.A.A. website during the race this feature will be the average pace up to each runner's most recent update.
+ **Proj Time:** In the scraped data this is empty because an Official Time has been registered. During the race this projected time will be the most recent average **Pace** times the total distance (26.2188 miles or 42.195 km).
+ **Overall:** This is the placement of each runner relative to all other runners, ordered by Official Time
+ **Gender:** This is the placement of each runner relative to all other runners with the same M/F designation.
+ **Division:** This is the placement of each runner relative to all other runners with the same M/F designation and the same Age Group.

The definition of the age groups and the qualification standards for each group were collected from  [the B.A.A.'s history of qualification standards](https://www.baa.org/races/boston-marathon/qualify/history-qualifying-times). Data was also collected from [the B.A.A.'s history of the official cut-off for entry relative to the qualification standard](https://www.baa.org/races/boston-marathon/qualify). For use in this project, this data is downloaded from [my GitHub Boston Marathon repository](https://github.com/delucj/BostonMarathonTimeProjections).

\newpage
## Summary of Key Preparation and Analysis Steps
The implementation of these models involves pre-processing the B.A.A. data into a more convenient format. Age Group and Wave information from the B.A.A. website is then joined into the scraped results data. Professional runners are removed from the set and the data sets are cleaned to remove any runners with missing or non-numeric time data. After cleaning the data, exploratory data analysis is performed on the 2015 and 2016 data to understand the key features of the marathon results in order to select the types of models used in this study (linear regression and regression tree random forests) and the form of the final model. The EDA is also used to identify a series of new features which can be calculated from existing features.

The entirety of the 2017 data set is completely withheld from this process for validation of the final. This decision of how to partition training and validation data is made because in production a predictive model will need to be prepared well in advance of race day (when we make a prediction for one runner at the 5km mark the other runners are also still on the course so we cannot have any finish time data to train on from this race). This inherently makes the task more challenging because the environmental conditions (temperature, precipitation, etc) will change from year to year and are not available as predictor variables in the training data.

To train the models, 67% of the data from each of the 2015 and 2016 races are combined into a single data set. The remaining 33% of the data is set aside as a testing set. The decision to train on 67% of the data is to mimic the training and validation split of 2 training races to predict one validation race. This decision ensures a large amount of testing data to help avoid over-fitting. Since there are about 52,000 runners between the two training races the training data set will remain large even with 33% of the data reserved for testing. After the models are trained and tested using this data the training and testing data sets are recombined to retrain the models before validation on the 2017 race.

Three types of models are developed in this project:

+ **Final Time Linear Regression** models are trained to make predictions at each check-point of when the runner will finish
  
  + These models are trained to reduce the biases observed in the Exploratory Data Analysis
  + These follow the average slow-down of runners from each check-point to the finish line observed in 2015 and 2016
+ **Next Check-Point Linear Regression Models** are used to predict the time that each runner will reach the next check-point
  
  + These models are intended to find signs of abnormal slow-down or speed-up seen in the EDA
  + When each runner gets to a check-point their time for the next check-point is predicted
  + When that runner reaches the next check-point the deviation between that time when a similar runner is expected to reach that check-point becomes a feature for use in the regression tree models

+ **Final Time Modification Regression Tree Random Forest Models** are developed to predict runner specific deviations from the predictions of the linear regression model
  + These models are intended to reduce the large variances observed in the EDA
  + Runners could have either a smooth slow-down type race where the final time will be best modeled as multiple of what would be naively expected or runners could have step function changes in their finish time associated with bathroom breaks, injuries or the dreaded "wall"
  + For step function type changes an additive error term makes more physical sense than a multiplier
  + At each check-point separate regression tree random forests are trained to model each of these types of behavior

The final model for each check-point is an average of modifications to the linear model from each of the two regression tree models.

\newpage
# Exploratiry Data Analysis and Modeling Methods
## Data Preparation

The Boston Marathon results include both professional runners and amateur runners. The performance of the professional runners on the course are not at all representative of how the vast majority of runners will run. The professional runners (any runner with a "F" bib or a bib number <100) are not running to finish a marathon, they are racing for prize money so they are likely to run relatively steady paces or drop out to compete another day. The first few results from the 2015 Boston Marathon illustrate that these runners are not representative of the three hour and forty-seven minute average time run by predominantly USA based runners who average about 40 years old:

```{r Dataset Example, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
## Just show the raw head
bm_2015_knit %>% select(Bib,Name,Age,M.F,Country,X5K,Half,X35K,Official.Time,Pace) %>%
  knitr::kable(padding = 1)

## If the user has the "plyr" package loaded it needs to be detached for the "dplyr" function n() to work here
#detach(package:plyr)
bm_2015_results %>% group_by(Country) %>%
  summarize(Runners = length(Bib),
            Mean.Finish = convert_min_to_racetime(mean(Final_Time)),
            Mean.Age = round(mean(Age),1) )%>%
  arrange(desc((Runners))) %>% filter(Runners > 100) %>% knitr::kable(padding = 1)

```

The data is cleaned by removing any runner with an elite bib and removing any runner with missing check-point or finish times (this project is not attempting to model the behavior of runners such as **[Rosie Ruiz](https://en.wikipedia.org/wiki/Rosie_Ruiz)**). The time data are then converted from a h:m:s format into minutes. Bib numbers are used assign each runner to a Wave. The B.A.A. projected finish times at each split are added as well as a few other calculated features such as pace from checkpoint to checkpoint both in units of minutes and normalized to the runner's pace over the first 10km of the course (these features are described in more detail in the **Model Development** section of this report).

```{r Plot Objects, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Create Plot Objects To Insert in Report
## These are some plots that I'm considering most strongly for the report
## Plots are assigned to objects to simplify the chunks at their insertion point in the markdown file
##
#########################################################

## General finish time visualization
gen_dist_2015_2016 <- ggarrange(plot_finish_gender_distribution(bm_2015_results,"2015") + xlab(""),
	plot_finish_gender_distribution(bm_2016_results,"2016"),ncol = 1, nrow = 2)


## 2016 had a more general trend toward slowing down in the second half
dist_add_slow_2015_2016 <- ggarrange(plot_splis_dist_5k_addative(bm_2015_results,2015)+xlab(""),
         plot_splis_dist_5k_addative(bm_2016_results,2016),
         ncol = 1, nrow = 2)

## 2016 had a more general trend toward slowing down in the second half
dist_mult_slow_2015_2016 <- ggarrange(plot_splis_dist_5k_multiplicative(bm_2015_results,2015,0.01)+xlab(""),
         plot_splis_dist_5k_multiplicative(bm_2016_results,2016,0.01),
         ncol = 1, nrow = 2)

## Slow-down effects, comparing additive and multiplicative
slow_down_compare <- ggarrange(plot_splis_dist_5k_addative(bm_2015_results,2015)+xlab("") +
                                 ggtitle("Slow-Down Adder | 2015"),
                               plot_splis_dist_5k_multiplicative(bm_2015_results,2015,0.01)+xlab("") +
                                 ggtitle("Slow-Down Multiplier | 2015"),
                               plot_splis_dist_5k_addative(bm_2016_results,2016) +
                                 ggtitle("Slow-Down Adder | 2016"),
                               plot_splis_dist_5k_multiplicative(bm_2016_results,2016,0.01) +
                                 ggtitle("Slow-Down Multiplier | 2015"),
                               ncol = 2, nrow = 2)

## Each wave appears to slow down a bit more than the previous with increasing variation
dist_add_slow_half <- ggarrange(plot_neg_pos_split(bm_2015_results,2015) + xlab(""),
	plot_neg_pos_split(bm_2016_results,2016),ncol = 1, nrow = 2)

## There appear to be general trends in slow down which differ by wave at the 5k
scat_mult_slow_wave_gen_5k <- ggarrange(plot_resids_by_age_multiplicative("5k",bm_2015_results,
                                                                       2015,bm_2015_results$PaceAt5K) +
                                       theme(legend.position = "none"),
                                     plot_resids_by_age_multiplicative("5k",bm_2016_results,
                                                                       2016,bm_2016_results$PaceAt5K) + ylab(""),
                                     ncol = 2, nrow = 1)

## The pace ratio appears to reverse by the half...this may suggest that older runners may settle into their final pace better than younger?
scat_mult_slow_wave_gen_half <- ggarrange(plot_resids_by_age_multiplicative("Half",bm_2015_results,
                                                                       2015,bm_2015_results$PaceAtHalf) +
                                       theme(legend.position = "none"),
                                     plot_resids_by_age_multiplicative("Half",bm_2016_results,
                                                                       2016,bm_2016_results$PaceAtHalf) + ylab(""),
                                     ncol = 2, nrow = 1)

## Look at slow down at 5k vs the half marathon
split_compare <- ggarrange(scat_mult_slow_wave_gen_5k,scat_mult_slow_wave_gen_half,ncol = 1, nrow=2)

```

```{r Generate Training and Testing Sets, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Create Training and Testing Sets
## Models will be built with a training set and tuned to give the best performance on a random test set
## Since 2015 and 2016 are characteristically different we will mix both into training and testing sets
## The expectation is that the linear models will somewhat split the difference between 2015 and 2016
## The random forest models will be expected to learn from the data what are the signs of slowdown to predict either
## The race year will be withheld from all models since we will need to predict 2017 with no 2017 information
##
#########################################################

## Make testing and training data from 2015 and 2016 data
## 67% training and 33% testing is selected to mirror the 2-races for development (2015 + 2016) and 1-race for validation (2017) split

## Make training and testing sets from 2015 data
test_train_list <- make_train_test_sets(bm_2015_results,0.33,42)
training <- test_train_list[[1]]
testing <- test_train_list[[2]]

## Append on training and testing data from 2016
test_train_list <- make_train_test_sets(bm_2016_results,0.33,42)
training <- rbind(training,test_train_list[[1]])
testing <- rbind(testing,test_train_list[[2]])

## The after tuning on the training set to optimize results on the testing set we will retrain on the combined data set
final_training <- rbind(training,testing)
```

```{r Linear Models, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Bias Reduction: Linear Models
## The linear models will be very simple.
## Their job is to reduce the bias so that our more complex models can focus primarily on the variance.
## Since we are not tuning these basic models we will jump straight to training on the final_training dataset
##
#########################################################

## Linear Prediction Model Training -- Final Time Predictions
## The Wave and gender are selected for categorical inclusion with the raw time based on the EDA
mod_lm_05 <- train(Final_Time ~ KM05_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_10 <- train(Final_Time ~ KM10_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_15 <- train(Final_Time ~ KM15_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_20 <- train(Final_Time ~ KM20_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_Half <- train(Final_Time ~ Half_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_25 <- train(Final_Time ~ KM25_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_30 <- train(Final_Time ~ KM30_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_35 <- train(Final_Time ~ KM35_Time + Wave + M.F,data = final_training,method = "lm")
mod_lm_40 <- train(Final_Time ~ KM40_Time + Wave + M.F,data = final_training,method = "lm")

## Linear Prediction Model Training -- Next Check-Point Time Predictions
## At each check-point we will also predict the time the runner will reach the next check-point
## Once the runner arrives at the next check point their residual to the prediction can be used as a feature in the Random Forest models
pred10k_lm <- train(KM10_Time ~ KM05_Time + Wave + M.F,data = final_training,method = "lm")
pred15k_lm <- train(KM15_Time ~ KM10_Time + Wave + M.F,data = final_training,method = "lm")
pred20k_lm <- train(KM20_Time ~ KM15_Time + Wave + M.F,data = final_training,method = "lm")
pred_half_lm <- train(Half_Time ~ KM20_Time + Wave + M.F,data = final_training,method = "lm")
pred25k_lm <- train(KM25_Time ~ Half_Time + Wave + M.F,data = final_training,method = "lm")
pred30k_lm <- train(KM30_Time ~ KM25_Time + Wave + M.F,data = final_training,method = "lm")
pred35k_lm <- train(KM35_Time ~ KM30_Time + Wave + M.F,data = final_training,method = "lm")
pred40k_lm <- train(KM40_Time ~ KM35_Time + Wave + M.F,data = final_training,method = "lm")

training <- add_mid_race_miss(training)
testing <- add_mid_race_miss(testing)
final_training <- add_mid_race_miss(final_training)

```


```{r Linear Models 2, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
#########################################################
##
## Variance Reduction: Regression Tree Random Forest Models
## We will be using regression tree random forest models to predict the residuals to or ratio to the linear fit models
## The Regression Tree model is selected due to the high degree of variance seen in the visualizations and the posibility of data clusters
## The Random Forest is selected to use randomization to target better out-of-sample performance
##
#########################################################

## One class of regression tree random forest will attempt to predict addative residuals to the linear model predictions
## To train on these we will predict the final time for each runner in the training set and calculate the difference between the actual and predicted times
training$lm_resids05 <- training$Final_Time - predict(mod_lm_05,training)
training$lm_resids10 <- training$Final_Time - predict(mod_lm_10,training)
training$lm_resids15 <- training$Final_Time - predict(mod_lm_15,training)
training$lm_resids20 <- training$Final_Time - predict(mod_lm_20,training)
training$lm_residsHalf <- training$Final_Time - predict(mod_lm_Half,training)
training$lm_resids25 <- training$Final_Time - predict(mod_lm_25,training)
training$lm_resids30 <- training$Final_Time - predict(mod_lm_30,training)
training$lm_resids35 <- training$Final_Time - predict(mod_lm_35,training)
training$lm_resids40 <- training$Final_Time - predict(mod_lm_40,training)

## The second class of regression tree random forest will attempt to predict the ratio of the final time to the time predicted from the linear model
## To train on these we will predict the final time for each runner in the training set and divide the actual final time by this prediction
training$lm_rato_05 <- training$Final_Time / predict(mod_lm_05,training)
training$lm_rato_10 <- training$Final_Time / predict(mod_lm_10,training)
training$lm_rato_15 <- training$Final_Time / predict(mod_lm_15,training)
training$lm_rato_20 <- training$Final_Time / predict(mod_lm_20,training)
training$lm_rato_Half <- training$Final_Time / predict(mod_lm_Half,training)
training$lm_rato_25 <- training$Final_Time / predict(mod_lm_25,training)
training$lm_rato_30 <- training$Final_Time / predict(mod_lm_30,training)
training$lm_rato_35 <- training$Final_Time / predict(mod_lm_35,training)
training$lm_rato_40 <- training$Final_Time / predict(mod_lm_40,training)

```

```{r Random Forest 5km, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 5km: Random Forests
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 5km mark
mod_rf_05k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_05k_add.rds","mod_rf_05k_add.rds")

# This is the ratio error Random Forest model at the 5km mark
mod_rf_05k_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_05k_mult.rds","mod_rf_05k_mult.rds")

```

```{r Random Forest 10km, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 10km: Random Forests
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
## 
################################

# This is the addition residual Random Forest model at the 10km mark
mod_rf_10k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_10k_add.rds","mod_rf_10k_add.rds")

# This is the ratio error Random Forest model at the 10km mark
mod_rf_10k_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_10k_mult.rds","mod_rf_10k_mult.rds")

```

```{r Random Forest 15km, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 15km: Random Forests
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 15km mark
mod_rf_15k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_15k_add.rds","mod_rf_15k_add.rds")

# This is the ratio error Random Forest model at the 15km mark
mod_rf_15k_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_15k_mult.rds","mod_rf_15k_mult.rds")

```

```{r Random Forest 20km, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 20km: Random Forests
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 20km mark
mod_rf_20k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_20k_add.rds","mod_rf_20k_add.rds")

# This is the ratio error Random Forest model at the 20km mark
mod_rf_20k_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_20k_mult.rds","mod_rf_20k_mult.rds")

```

``` {r Random Forest Half, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## Half Marathon: Random Forest
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the Half Marathon mark
mod_rf_half_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_half_add.rds","mod_rf_half_add.rds")

# This is the ratio error Random Forest model at the Half Marathon mark
mod_rf_half_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_half_mult.rds","mod_rf_half_mult.rds")

```

``` {r Random Forest at 25K, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 25kM: Random Forest
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 25k mark
mod_rf_25k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_25k_add.rds","mod_rf_25k_add.rds")

# This is the ratio error Random Forest model at the 25km mark
mod_rf_25_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_25_mult.rds","mod_rf_25_mult.rds")

```

``` {r Random Forest at 30K, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 30kM: Random Forest
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 30k mark
mod_rf_30k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_30k_add.rds","mod_rf_30k_add.rds")

# This is the ratio error Random Forest model at the 30km mark
mod_rf_30_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_30_mult.rds","mod_rf_30_mult.rds")

```

``` {r Random Forest at 35K, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 35kM: Random Forest
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 35k mark
mod_rf_35k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_35k_add.rds","mod_rf_35k_add.rds")

# This is the ratio error Random Forest model at the 35km mark
mod_rf_35_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_35_mult.rds","mod_rf_35_mult.rds")

```

``` {r Random Forest at 40K, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
################################
##
## 40kM: Random Forest
## This model is downloaded from GitHub
## Use the R-script for this project if you want to regenerate the model
##
################################

# This is the addition residual Random Forest model at the 40k mark
mod_rf_40k_add <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_40k_add.rds","mod_rf_40k_add.rds")

# This is the ratio error Random Forest model at the 40km mark
mod_rf_40_mult <- read_rds_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/mod_rf_40_mult.rds","mod_rf_40_mult.rds")

```

\newpage
## Data Visualization

Although the standard deviation of the finish time is similar in 2015 and 2016, the average is almost 10 minutes faster in 2015 and the width of the distribution of how much runners slow down from their 5km time is broader in 2016. These observations illustrate that the location of the distribution of finish times is not well known for an arbitrary race and the shape and location of the slow-down distributions will also be variable. This means that while a linear regression model may remove some of the general slow-down bias a more flexible model is necessary to discover the factors which predict this variation.

```{r Plot General Distribution, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE,out.width="75%",out.height="72%",fig.align='center'}
gen_dist_2015_2016
```

```{r Plot Slowdown Distribution, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE,out.width="75%",out.height="72%",fig.align='center'}
slow_down_compare
```

\newpage
The comparison of the degree to which runners slow down from their 5km pace and half-marathon pace as a function of age, shows that there are qualitatively similar trends from year to year split by gender and wave. This means that Wave, Gender and Age could be included in the Linear Regression Models which will be intended to reduce the biases in the predictions. We can see from this same plot that although these trends are qualitatively similar and biased away from the nominal value of 1 (the B.A.A. baseline model always predicts a Pace Ratio of 1 at all splits) the variance around each trend is very large: a variance reduction model will need to be combined with a bias reduction model to optimize the RMSE.

```{r Plot of Age and Slowdown, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE,fig.align='center'}
split_compare
```
\newpage
A series of 3D surface plots were created to visualize effects such as age on the final time at various check-points in combination with other time-based factors. An example is shown below looking for whether or not Age modified the relationship between Bib number and Finish Time. In this example there are clusters or waves in the data which will not be well captured by linear regression models. Furthermore, in this example it appears that most of the information about Finish Time from Age is already captured by Bib Number. In each case, the Age feature does not provide a strong effect which is not already captured in some other metric. For this reason Age is excluded from the bias reduction linear regression models.

```{r Plot Surface Response, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE,fig.align='left'}
bib_age_surface(bm_2015_results,2015,F,30)
```

Since there are signs of clusters or non-linear relationships between Age and Bib number both features are included in the initial regression tree models. Due to the high degree of unexplained variance in the simple visualizations, new calculated features must be added to the Boston Marathon results data to extend the flexibility of the random forest regression tree models. These features will be intended to look for runner speed changes along the course so that runners who have similar velocity derivatives but dissimilar velocities can be clustered and analyzed using the regression tree models responsible for reducing the final variance. The calculations involved in exposing these features are described in the **Model Development** section of this report.

\newpage
## Model Development

The first step in the development of this project was to build two sets of 9 linear regression models. The first set of models are the *bias reduction models* which are based on the EDA observations that runners generally have slower finish times than predicted by assumption of a constant pace. At each check-point along the course a linear regression model is trained to predict the Final Time from the runner's check-point time, Gender and Wave. These models are selected to be highly simplistic to train rapidly and remove the gross biases to simplify the task of the more complex regression tree models.

Since the linear regression models are not tuned they are trained directly on the combination of all of the 2015 and 2016 results after spot-checking the significance of the feature coefficients on the training data subset and that the predictions on the test set have lower RMSE than the B.A.A. baseline model.

Using the same features, a second linear regression model was trained for each check-point to predict the time at which each runner would reach the next check-point. At each check-point after the first (5km), the actual check-point time is compared to the time predicted from the previous check point to get a residual in minutes (actual - predicted) and a ratio (actual / predicted); both of these features are added to the dataframe for use in the regression tree random forest variance reduction models.

For example, let us assume a runner arrived at the 10km point at 0:44:21 and the linear model predicted that this runner would arrive at the 15km check-point at 1:07:22 but the runner actually arrived at 1:06:45.

+ In this example this runner would have an additive residual of **1:06:45 - 1:07:22 = -0.62 minutes**
+ This runner would have a ratio of **1:06:45 / 1:07:22 = 0.991**
+ This runner's pace was **0:07:09 per mile** for the first 10km
+ The runner's pace from the 10km mark to the 15km mark was **(1:06:45 - 0:44:21) / 3.11 = 0:07:12**
+ This means that this runner would also have a normalized pace at 15km of **0:07:12 / 0:07:09 = 1.009**

At each check-point two regression tree random forest models are trained using the training data subset of 2015 and 2016 results. Early in the race all available features are provided to the model; as the race progresses the variable importance is evaluated at each check-point and the least important variables are excluded from the next check-point's model and new features that become available at the new check-point are added. Each Random Forest model has its node size tuned on the training subset to optimize for the lowest estimated Out-Of-Sample RMSE on the reserved testing subset of 2015 and 2016 data.

The method selected to optimize node size is a for loop which iterates over a range of node sizes trains a model on the training subset, makes predictions on the testing subset and appends the estimated Out-of-Sample (O.o.S.) RMSE to a results vector. Due the slow process of tuning a random forest model (particularly with many trees and/or with small node sizes) this optimization uses forests of 75 trees rather than the 500 tree default. After the 5km check-point a narrower, course mesh of node sizes are used with the optimized node size from the previous model used to estimate the range of node sizes over which the next model is optimized. The node size with the lowest estimated O.o.S. RMSE and its closest neighbors are passed with the RMSE values to a function which calculates the coefficients for a quadratic fit to the data. The function then solves for the nose size where the derivative of the quadratic fit is 0 and returns the closest integer node size. With this optimized node size a while loop is used to grow the forest in increments of 25 trees until the RMSE stops decreasing by >1% per iteration. After the node size and the number of trees are optimized on the training and testing data both data subsets are recombined and the finalized random forest of regression trees is trained on the complete 2015 and 2016 results.

Speaking from the personal experience of having marathon finish times range from 3:02:47 to 4:57:39, having dropped out once with a broken leg and spent 20 minutes in a medical tent the magnitude of the most extreme residuals will be inherently unpredictable (though the probability of an extreme residual may be predictable). For this reason, I never allowed the minimum node size to be below 10 runners despite the function's default node size of 5 observations. The node size calculation is bounded to force the calculation away from arbitrarily small or large node sizes: if the minimum is found at either the largest or smallest node size tested then that node size is used rather than extrapolating outside the known data.

At each check-point, two random forest models are optimized and trained using the method described above. One model predicts the additive residual (actual final time - predicted final time) to the linear regression prediction using such features as check-point time, Age, Wave, Gender, check-point pace normalized to 10km and the observed errors between predicted and actual arrival at various check points. The second model predicts the ratio (actual finish time / predicted finish time) using similar features as the additive model.

The additive model is intended to look for signs of injury, the "wall", walk-breaks or other such fixed time type delays while the multiplicative model is intended to mimic more gradual slowdown behaviors associated with exhaustion. The two model effects are averaged with equal weights.

The final model for each check-point takes the form:
$$\widehat{Y_{cp}}=\left( \frac{(\widehat{Y_{lm_{cp}}}+\widehat{A_{rf_{cp}}})+(\widehat{Y_{lm_{cp}}}*\widehat{R_{rf_{cp}}})}{2} \right)$$
Where:
$$\widehat{Y_{cp}} = Final\ Race\ Time\ Prediction \ at\ Checkpoint\ (cp)$$
$$\widehat{Y_{lm_{cp}}} = Linear\ Regression\ Model\ Prediction\ of\ Final\ Race\ Time\ at\ Checkpoint\ (cp)$$
$$\widehat{A_{rf_{cp}}} = Random\ Forest\ Model\ Prediction\ of\ Addition\ Residual\ to\ Linear\ Model\ Predicted\ Time\ at\ Checkpoint\ (cp)$$
$$\widehat{R_{rf_cp}} = Random\ Forest\ Model\ Prediction\ of\ Final\ Time\ Divided\ By\ Linear\ Model\ Predicted\ Time\ at\ Checkpoint\ (cp)$$
The runner's pace normalized to their 10km pace and the degree to which they beat or missed the time for their recent check-points as predicted at the previous check-points are typically the most important features. An example below shows the relative feature importance analysis at 25km which resulted in the exclusion of Bib, Pace at 5k from the 30km models.

```{r Display Variable Importance, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE,out.width="75%",out.height="75%",fig.align='center'}
ggarrange(plot_rf_imp(mod_rf_25_mult,"Ratio Model at 25km"),plot_rf_imp(mod_rf_25k_add,"Residual Model at 25km"),ncol = 2, nrow = 1)
```

The "MissAt..." features are the residuals and ratios of the runner's actual arrival time at the listed check-point compared to their predicted arrival time. The "NrmPace_" features are the pace from the previous check-point to the listed check-point normalized to the runners pace over the first 10km of the course. 

\newpage
# Results and Conclusions
## Results

```{r Model Validation, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
validation <- read_csv_from_github("https://raw.githubusercontent.com/delucj/BostonMarathonTimeProjections/main/marathon_results_2017.csv","marathon_results_2017.csv")

validation <- pre_process_data(validation,2017)
validation <- add_mid_race_miss(validation)

validation_naive <- get_naive_performance(validation)
validation_lm <- get_linear_performance(validation)
validation_smart <- get_rf_performance(validation)
validation_smart$Improvement <- 100 * (validation_naive$Naive.RMSE - validation_smart$RF.RMSE) / validation_naive$Naive.RMSE

# Plot the naive mode RMSE at each split
overall <- ggplot() + geom_point(aes(x=Distance,y=Naive.RMSE),data = validation_naive,color="black") +
	geom_line(aes(x=Distance,y=Naive.RMSE),data = validation_naive,color="black") +
	geom_point(aes(x=Distance,y=RF.RMSE),data = validation_smart,color="blue") +
	geom_line(aes(x=Distance,y=RF.RMSE),data = validation_smart,color="blue") +
  geom_point(aes(x=Distance,y=Linear.RMSE),data = validation_lm,color="red") +
	geom_line(aes(x=Distance,y=Linear.RMSE),data = validation_lm,color="red") +
	xlab("Race Distance Completed (km)") + ylab("Model RMSE (minutes)") + 
	ggtitle("Model Validation | 2017") +
	annotate("text",x=27,y=34,label = "BAA Baseline Model",color="black") +
	annotate("text",x=27,y=32,label = "Linear Model",color="red") +
  annotate("text",x=27,y=30,label = "Lm and RF Ensemble Model",color="blue") +
  xlim(c(0,42)) + ylim(c(0,35)) + 
  geom_rect(aes(xmin = 18,xmax = 27, ymin = 8, ymax = 24),
		color = "black",fill = "light green", alpha=0.2)

improved <- ggplot() + geom_point(aes(x=Distance,y=Improvement),data = validation_smart,color="black") +
	geom_line(aes(x=Distance,y=Improvement),data = validation_smart,color="black") +
	xlab("Race Distance Completed (km)") + ylab("Improvement over Baseline (%)") + 
  xlim(c(0,42)) + ylim(c(0,100)) + 
  geom_rect(aes(xmin = 18,xmax = 27, ymin = 40, ymax = 55),
		color = "black",fill = "light green", alpha=0.2) +
  annotate("text",x=22.5,y=60,
		label = "Key Prediction Check-Points",color="black") +
  ggtitle("Improvement over Baseline")

```

The model is validated using the entire 2017 Boston Marathon results dataset. The linear regression models show a significant improvement in RMSE over the entire race course. At the first and last check-points the Random Forest model only offers modest benefits over the linear regression model. This is because at 5km the data set does not have a sufficiently rich set of predictor features to improve the prediction and at 40km the race is almost over so there is little excess variation which can be modeled. Over the more important middle section of the race, the optimized ensemble model outperforms the Baseline model by approximately 47.5%. At the half marathon the RMSE for the ensemble model is approximately 18% lower than for the linear regression model.

```{r Show Results, echo=FALSE,error=FALSE,warning=FALSE, message=FALSE}
ggarrange(overall,improved,ncol = 2, nrow = 1)
```

\newpage
## Conclusions, Limitations and Future Work
This project details the development of a machine learning model which significantly improves the accuracy of finish time predictions at each of 9 check-points along the Boston Marathon course over the existing baseline model. An average improvement of **>40%** is demonstrated in out-of-sample validation using the 2017 marathon results which were completely withheld from the model training. At the most important splits at the center of the race (when spectators will be making their decisions about when to head to the finish line) the RMSE is improved by **~47.5%**.

The model used in this project is an ensemble method which uses a linear regression model to reduce the baseline bias combined with a pair of Random Forest models which are used to reduce the variance left over after applying the linear regression model.

While the predictions from this model may be of interest to runners on the course for an improved understanding of their likelihood of meeting some personal finish time goal the true impact of this work is the improved knowledge given to race organizes about when runners are expected to finish and the improved RMSE of that prediction. By integrating these predictions and tightened confidence intervals with security at restricted areas spectators can be allowed into these prime viewing locations within a narrower window. By using this sort of spectator management the risks associated with highly congested areas can be reduced. This may potentially allow major marathon events like the Boston Marathon to resume in the age of Social Distancing.

A manageable but significant limitation to the existing model is that it sees only chip-time. This is the time at which runners cross timing mats along the course relative to the time when they crossed the start line. In future work, if the B.A.A. makes gun-time available (the absolute time from when the race first started, independent of when the runner crosses the start line) to the model then further separation of spectators can be achieved. For example, a runner who starts at the back of Wave 4 may cross the start line 90 minutes or more after a runner who starts at the front of Wave 1 so even if their projected finish times are the same, the actual projected time of day when the two runners are expected to reach the finish line could be quite different.

The inclusion of gun-time or actual time of day in the historical data could also allow for temperature and precipitation data to be merged into the training data allowing for weather forecasts to improve predictions at early-race check points and actual measured temperatures and precipitation levels to be used to improve predictions at later check points.